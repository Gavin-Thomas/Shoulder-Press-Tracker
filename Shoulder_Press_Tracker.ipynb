{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shoulder Press Tracker"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[My Final Project in depth explanation here](https://gavin-thomas.github.io/Shoulder-Press.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### #1 Install and Import Necessary Libraries\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First I installed mediapipe, which is a general \"detection\" pre-built deep-learning (computer vision) library library\n",
    "pip install mediapipe\n",
    "\n",
    "\n",
    "# Next I installed opencv, which is a computer vision library that allows us to use our webcam.\n",
    "pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "# Numpy for Trig :)\n",
    "import numpy as np\n",
    "\n",
    "# A mediapipe solution is like a model that we are grabbing\n",
    "mp_drawing=mp.solutions.drawing_utils\n",
    "mp_pose=mp.solutions.pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 16\u001b[0m\n\u001b[1;32m     12\u001b[0m     cv2\u001b[39m.\u001b[39mimshow(\u001b[39m'\u001b[39m\u001b[39mMediapipe Feed\u001b[39m\u001b[39m'\u001b[39m,frame)\n\u001b[1;32m     14\u001b[0m     \u001b[39m# check if we try to break out of screen, this function helps us break out of the while loop\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     \u001b[39m# 0xFF helps us figure out which key we are hitting on our keyboard\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m     \u001b[39mif\u001b[39;00m cv2\u001b[39m.\u001b[39;49mwaitKey(\u001b[39m10\u001b[39;49m) \u001b[39m&\u001b[39m \u001b[39m0xFF\u001b[39m \u001b[39m==\u001b[39m \u001b[39mord\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mq\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m     17\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[39m# if we break out of our video feed we will release our video feed and destroy windows\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Here is our VIDEO FEED\n",
    "\n",
    "# Setup video capture device\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# This loops through our video feed \n",
    "while cap.isOpened():\n",
    "    # cap.read is like us getting the current feed from webcam\n",
    "    # frame variable gives us the image from our webcam, ret is return var\n",
    "    ret,frame = cap.read()\n",
    "    #pipe through mediapipe feed, and image from webcam (frame)\n",
    "    cv2.imshow('Mediapipe Feed',frame)\n",
    "\n",
    "    # check if we try to break out of screen, this function helps us break out of the while loop\n",
    "    # 0xFF helps us figure out which key we are hitting on our keyboard\n",
    "    if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# if we break out of our video feed we will release our video feed and destroy windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### #2 Make Detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 21\u001b[0m\n\u001b[1;32m     17\u001b[0m image\u001b[39m.\u001b[39mflags\u001b[39m.\u001b[39mwriteable \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[39m# This line makes the detection (we set up our pose model before)\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[39m# We store detections inside results\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m results \u001b[39m=\u001b[39m pose\u001b[39m.\u001b[39;49mprocess(image)\n\u001b[1;32m     23\u001b[0m \u001b[39m# Write up image then colour the image again back to normal image format\u001b[39;00m\n\u001b[1;32m     24\u001b[0m image\u001b[39m.\u001b[39mflags\u001b[39m.\u001b[39mwriteable \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/mediapipe/python/solutions/pose.py:185\u001b[0m, in \u001b[0;36mPose.process\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mprocess\u001b[39m(\u001b[39mself\u001b[39m, image: np\u001b[39m.\u001b[39mndarray) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m NamedTuple:\n\u001b[1;32m    165\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Processes an RGB image and returns the pose landmarks on the most prominent person detected.\u001b[39;00m\n\u001b[1;32m    166\u001b[0m \n\u001b[1;32m    167\u001b[0m \u001b[39m  Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[39m         \"enable_segmentation\" is set to true.\u001b[39;00m\n\u001b[1;32m    183\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m   results \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mprocess(input_data\u001b[39m=\u001b[39;49m{\u001b[39m'\u001b[39;49m\u001b[39mimage\u001b[39;49m\u001b[39m'\u001b[39;49m: image})\n\u001b[1;32m    186\u001b[0m   \u001b[39mif\u001b[39;00m results\u001b[39m.\u001b[39mpose_landmarks:  \u001b[39m# pytype: disable=attribute-error\u001b[39;00m\n\u001b[1;32m    187\u001b[0m     \u001b[39mfor\u001b[39;00m landmark \u001b[39min\u001b[39;00m results\u001b[39m.\u001b[39mpose_landmarks\u001b[39m.\u001b[39mlandmark:  \u001b[39m# pytype: disable=attribute-error\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/mediapipe/python/solution_base.py:365\u001b[0m, in \u001b[0;36mSolutionBase.process\u001b[0;34m(self, input_data)\u001b[0m\n\u001b[1;32m    359\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    360\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_graph\u001b[39m.\u001b[39madd_packet_to_input_stream(\n\u001b[1;32m    361\u001b[0m         stream\u001b[39m=\u001b[39mstream_name,\n\u001b[1;32m    362\u001b[0m         packet\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_packet(input_stream_type,\n\u001b[1;32m    363\u001b[0m                                  data)\u001b[39m.\u001b[39mat(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_simulated_timestamp))\n\u001b[0;32m--> 365\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_graph\u001b[39m.\u001b[39;49mwait_until_idle()\n\u001b[1;32m    366\u001b[0m \u001b[39m# Create a NamedTuple object where the field names are mapping to the graph\u001b[39;00m\n\u001b[1;32m    367\u001b[0m \u001b[39m# output stream names.\u001b[39;00m\n\u001b[1;32m    368\u001b[0m solution_outputs \u001b[39m=\u001b[39m collections\u001b[39m.\u001b[39mnamedtuple(\n\u001b[1;32m    369\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mSolutionOutputs\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output_stream_type_info\u001b[39m.\u001b[39mkeys())\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Here is our VIDEO FEED\n",
    "\n",
    "# Setup video capture device\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Setup our detection confidence and our tracking confidence\n",
    "with mp_pose.Pose(min_detection_confidence=0.5,min_tracking_confidence=0.5) as pose:\n",
    "    # Loop through our video feed \n",
    "    while cap.isOpened():\n",
    "        # cap.read is like us getting the current feed from webcam\n",
    "        # frame variable gives us the image from our webcam, ret is return var\n",
    "        ret,frame = cap.read()\n",
    "\n",
    "        # Let's try and make some detections now...\n",
    "        # First we recolour our image\n",
    "        image = cv2.cvtColor(frame,cv2.COLOR_BGR2RGB)\n",
    "        image.flags.writeable = False\n",
    "\n",
    "        # This line makes the detection (we set up our pose model before)\n",
    "        # We store detections inside results\n",
    "        results = pose.process(image)\n",
    "\n",
    "        # Write up image then colour the image again back to normal image format\n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        # Let's Render our image detection now\n",
    "        # No need to draw point, by point, just use media pipe drawing utilities\n",
    "        mp_drawing.draw_landmarks(image,results.pose_landmarks,mp_pose.POSE_CONNECTIONS,\n",
    "                                # Dot colour and specifications\n",
    "                                mp_drawing.DrawingSpec(color=(245,117,66),thickness=2,circle_radius=2),\n",
    "                                # Line colour and specifications\n",
    "                                mp_drawing.DrawingSpec(color=(245,66,230),thickness=2,circle_radius=2))\n",
    "\n",
    "\n",
    "        #pipe through mediapipe feed, and image from webcam (frame)\n",
    "        cv2.imshow('Mediapipe Feed',image)\n",
    "\n",
    "        # check if we try to break out of screen, this function helps us break out of the while loop\n",
    "        # 0xFF helps us figure out which key we are hitting on our keyboard\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "# if we break out of our video feed we will release our video feed and destroy windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### #3 Determining Joints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is our VIDEO FEED\n",
    "\n",
    "# Setup video capture device\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Setup our detection confidence and our tracking confidence\n",
    "with mp_pose.Pose(min_detection_confidence=0.5,min_tracking_confidence=0.5) as pose:\n",
    "    # Loop through our video feed \n",
    "    while cap.isOpened():\n",
    "        # cap.read is like us getting the current feed from webcam\n",
    "        # frame variable gives us the image from our webcam, ret is return var\n",
    "        ret,frame = cap.read()\n",
    "\n",
    "        # Let's try and make some detections now...\n",
    "        # First we recolour our image\n",
    "        image = cv2.cvtColor(frame,cv2.COLOR_BGR2RGB)\n",
    "        image.flags.writeable = False\n",
    "\n",
    "        # This line makes the detection (we set up our pose model before)\n",
    "        # We store detections inside results\n",
    "        results = pose.process(image)\n",
    "\n",
    "        # Write up image then colour the image again back to normal image format\n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        # This block of code allows for the extraction of landmarks\n",
    "        # if we don't have detections or get an error we just pass through\n",
    "        try:\n",
    "            landmarks = results.pose_landmarks.landmark\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # Let's Render our image detection now\n",
    "        # No need to draw point, by point, just use media pipe drawing utilities\n",
    "        mp_drawing.draw_landmarks(image,results.pose_landmarks,mp_pose.POSE_CONNECTIONS,\n",
    "                                # Dot colour and specifications\n",
    "                                mp_drawing.DrawingSpec(color=(245,117,66),thickness=2,circle_radius=2),\n",
    "                                # Line colour and specifications\n",
    "                                mp_drawing.DrawingSpec(color=(245,66,230),thickness=2,circle_radius=2))\n",
    "\n",
    "\n",
    "        #pipe through mediapipe feed, and image from webcam (frame)\n",
    "        cv2.imshow('Mediapipe Feed',image)\n",
    "\n",
    "        # check if we try to break out of screen, this function helps us break out of the while loop\n",
    "        # 0xFF helps us figure out which key we are hitting on our keyboard\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "# if we break out of our video feed we will release our video feed and destroy windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many landmarks??\n",
    "len(landmarks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n"
     ]
    }
   ],
   "source": [
    "#print each landmark from particular landmark map\n",
    "for lndmrk in mp_pose.PoseLandmark:\n",
    "    print(lndmrk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "x: 0.7799567\n",
       "y: 0.84146\n",
       "z: -0.41408777\n",
       "visibility: 0.99849945"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Access the left shoulder landmark as loop and display it from array\n",
    "landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PoseLandmark.LEFT_SHOULDER: 11>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What number is the LEFT shoulder in our array of landmarks?\n",
    "mp_pose.PoseLandmark.LEFT_SHOULDER"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### #4 Calculating Angles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define a function to calculate the angle between the left hip, left shoulder, and left elbow\n",
    "def calculate_angle(a,b,c):\n",
    "    a = np.array(a) # First Joint\n",
    "    b = np.array(b) # Middle Joint\n",
    "    c = np.array(c) # End Joint\n",
    "\n",
    "    # Now lets do some Trigonometry!\n",
    "    radians = np.arctan2(c[1]-b[1], c[0]-b[0]) - np.arctan2(a[1]-b[1], a[0]-b[0])\n",
    "    angle = np.abs(radians*180.0/np.pi)\n",
    "    \n",
    "    # Our hinge joints cannot go more than 360 degrees hence this equation\n",
    "    if angle >180.0:\n",
    "        angle = 360-angle\n",
    "        \n",
    "    return angle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Left Shoulder x and y values\n",
    "hip = [landmarks[mp_pose.PoseLandmark.LEFT_HIP.value].x,landmarks[mp_pose.PoseLandmark.LEFT_HIP.value].y]\n",
    "shoulder = [landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value].x,landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value].y]\n",
    "elbow = [landmarks[mp_pose.PoseLandmark.LEFT_ELBOW.value].x,landmarks[mp_pose.PoseLandmark.LEFT_ELBOW.value].y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.7262412309646606, 0.8725166916847229],\n",
       " [0.8130804300308228, 1.3414616584777832],\n",
       " [0.8314734697341919, 1.7969979047775269])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hip, shoulder, elbow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15.541339769588362"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_angle(hip, shoulder, elbow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's just quickly find out what my width and height of video feed is!\n",
    "# Initialize video capture device\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Get width and height of video feed\n",
    "width = cap.get(cv2.CAP_PROP_FRAME_WIDTH)\n",
    "height = cap.get(cv2.CAP_PROP_FRAME_HEIGHT)\n",
    "\n",
    "# Print width and height of video feed\n",
    "print(f\"Video feed width: {width}, height: {height}\")\n",
    "\n",
    "# Release video capture device\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is our VIDEO FEED\n",
    "\n",
    "# Setup video capture device\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Setup our detection confidence and our tracking confidence\n",
    "with mp_pose.Pose(min_detection_confidence=0.5,min_tracking_confidence=0.5) as pose:\n",
    "    # Loop through our video feed \n",
    "    while cap.isOpened():\n",
    "        # cap.read is like us getting the current feed from webcam\n",
    "        # frame variable gives us the image from our webcam, ret is return var\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        # Let's try and make some detections now...\n",
    "        # First we recolour our image\n",
    "        image = cv2.cvtColor(frame,cv2.COLOR_BGR2RGB)\n",
    "        image.flags.writeable = False\n",
    "\n",
    "        # This line makes the detection (we set up our pose model before)\n",
    "        # We store detections inside results\n",
    "        results = pose.process(image)\n",
    "\n",
    "        # Write up image then colour the image again back to normal image format\n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        # This block of code allows for the extraction of landmarks\n",
    "        # if we don't have detections or get an error we just pass through\n",
    "\n",
    "        try:\n",
    "            landmarks = results.pose_landmarks.landmark\n",
    "            \n",
    "            # Get the X and Y values of the hip, shoulder, and elbow\n",
    "            hip = [landmarks[mp_pose.PoseLandmark.LEFT_HIP.value].x,landmarks[mp_pose.PoseLandmark.LEFT_HIP.value].y]\n",
    "            elbow = [landmarks[mp_pose.PoseLandmark.LEFT_ELBOW.value].x,landmarks[mp_pose.PoseLandmark.LEFT_ELBOW.value].y]\n",
    "            shoulder = [landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value].x,landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value].y]\n",
    "            \n",
    "            # Calculate angle\n",
    "            angle = calculate_angle(hip, shoulder, elbow)\n",
    "            \n",
    "            # Now let's visualize our angle data (pass through image from webcam and angle)\n",
    "            # Use array multiplication (grab elbow coordinate and multiply by my webcam dimensions)\n",
    "            # We do the multiplication to get the proper image dimensions (then we convert to a tuple which cv2 expects)\n",
    "            cv2.putText(image, str(angle), \n",
    "                           tuple(np.multiply(shoulder, [640, 480]).astype(int)), \n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2, cv2.LINE_AA\n",
    "                                )\n",
    "                       \n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # Let's Render our image detection now\n",
    "        # No need to draw point, by point, just use media pipe drawing utilities\n",
    "        mp_drawing.draw_landmarks(image,results.pose_landmarks,mp_pose.POSE_CONNECTIONS,\n",
    "                                # Dot colour and specifications\n",
    "                                mp_drawing.DrawingSpec(color=(245,117,66),thickness=2,circle_radius=2),\n",
    "                                # Line colour and specifications\n",
    "                                mp_drawing.DrawingSpec(color=(245,66,230),thickness=2,circle_radius=2))\n",
    "\n",
    "\n",
    "        #pipe through mediapipe feed, and image from webcam (frame)\n",
    "        cv2.imshow('Mediapipe Feed',image)\n",
    "\n",
    "        # check if we try to break out of screen, this function helps us break out of the while loop\n",
    "        # 0xFF helps us figure out which key we are hitting on our keyboard\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "# if we break out of our video feed we will release our video feed and destroy windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### #5 Dumbell Shoulder Press Tracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Count Shoulder Press Reps\n",
    "counter = 0\n",
    "# Stage represents the up or down phase of the curl\n",
    "stage = None\n",
    "\n",
    "\n",
    "\n",
    "# Here is our VIDEO FEED\n",
    "# Setup video capture device\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Setup our detection confidence and our tracking confidence\n",
    "with mp_pose.Pose(min_detection_confidence=0.5,min_tracking_confidence=0.5) as pose:\n",
    "    # Loop through our video feed \n",
    "    while cap.isOpened():\n",
    "        # cap.read is like us getting the current feed from webcam\n",
    "        # frame variable gives us the image from our webcam, ret is return var\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        # Let's try and make some detections now...\n",
    "        # First we recolour our image\n",
    "        image = cv2.cvtColor(frame,cv2.COLOR_BGR2RGB)\n",
    "        image.flags.writeable = False\n",
    "\n",
    "        # This line makes the detection (we set up our pose model before)\n",
    "        # We store detections inside results\n",
    "        results = pose.process(image)\n",
    "\n",
    "        # Write up image then colour the image again back to normal image format\n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        # This block of code allows for the extraction of landmarks\n",
    "        # if we don't have detections or get an error we just pass through\n",
    "\n",
    "        try:\n",
    "            landmarks = results.pose_landmarks.landmark\n",
    "            \n",
    "            # Get the X and Y values of the hip, shoulder, and elbow\n",
    "            hip = [landmarks[mp_pose.PoseLandmark.LEFT_HIP.value].x,landmarks[mp_pose.PoseLandmark.LEFT_HIP.value].y]\n",
    "            elbow = [landmarks[mp_pose.PoseLandmark.LEFT_ELBOW.value].x,landmarks[mp_pose.PoseLandmark.LEFT_ELBOW.value].y]\n",
    "            shoulder = [landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value].x,landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value].y]\n",
    "            \n",
    "            # Calculate angle\n",
    "            angle = calculate_angle(hip, shoulder, elbow).round(3)\n",
    "            \n",
    "            # Now let's visualize our angle data (pass through image from webcam and angle)\n",
    "            # Use array multiplication (grab elbow coordinate and multiply by my webcam dimensions)\n",
    "            # We do the multiplication to get the proper image dimensions (then we convert to a tuple which cv2 expects)\n",
    "            cv2.putText(image, str(angle), \n",
    "                           tuple(np.multiply(shoulder, [640, 480]).astype(int)), \n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2, cv2.LINE_AA\n",
    "                                )\n",
    "            # Now for some logical if statements for the shoulder press counter\n",
    "            if angle > 160:\n",
    "                stage = 'up'\n",
    "            if angle < 80 and stage == 'up':\n",
    "                stage = 'down'\n",
    "                counter += 1\n",
    "                print(counter)\n",
    "                       \n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # Now let's render the shoulder press ticker\n",
    "        # To do this let's first setup a status box (last line ='-1' fills box with colour)\n",
    "        cv2.rectangle(image, (0,0), (225,73), (245,117,16), -1)\n",
    "        \n",
    "        # Now for our REP data\n",
    "        cv2.putText(image,'Rep',(15,12),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,0,0), 1, cv2.LINE_AA)\n",
    "        cv2.putText(image, str(counter), (10,60),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 2, (255,255,0), 2, cv2.LINE_AA)            \n",
    "        \n",
    "        # Now for our STAGE data\n",
    "        cv2.putText(image,'Stage',(65,12),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,0,0), 1, cv2.LINE_AA)\n",
    "        cv2.putText(image, stage, (60,60),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 2, (255,255,0), 2, cv2.LINE_AA) \n",
    "\n",
    "        # Let's Render our image detection now\n",
    "        # No need to draw point, by point, just use media pipe drawing utilities\n",
    "        mp_drawing.draw_landmarks(image,results.pose_landmarks,mp_pose.POSE_CONNECTIONS,\n",
    "                                # Dot colour and specifications\n",
    "                                mp_drawing.DrawingSpec(color=(245,117,66),thickness=2,circle_radius=2),\n",
    "                                # Line colour and specifications\n",
    "                                mp_drawing.DrawingSpec(color=(245,66,230),thickness=2,circle_radius=2))\n",
    "\n",
    "\n",
    "        #pipe through mediapipe feed, and image from webcam (frame)\n",
    "        cv2.imshow('Mediapipe Feed',image)\n",
    "\n",
    "        # check if we try to break out of screen, this function helps us break out of the while loop\n",
    "        # 0xFF helps us figure out which key we are hitting on our keyboard\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "# if we break out of our video feed we will release our video feed and destroy windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
